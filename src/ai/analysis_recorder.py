#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI åˆ†æžç»“æžœå­˜å‚¨å’Œè®°å½•ç®¡ç†
è®°å½•æ‰€æœ‰é€šè¿‡ Dify å·¥ä½œæµå¤„ç†çš„æ–°é—»åˆ†æžç»“æžœ
é›†æˆäº†ç»“æž„åŒ–ç´¢å¼•ã€é…ç½®ç›‘æŽ§ã€ç¼“å­˜ç®¡ç†ç­‰åŠŸèƒ½
"""

import json
import os
import hashlib
import csv
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional
import sys

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from logger_config import get_logger

logger = get_logger(__name__)


class AnalysisRecorder:
    """è®°å½•å’Œç®¡ç†åˆ†æžç»“æžœ"""
    
    def __init__(self):
        """åˆå§‹åŒ–è®°å½•ç®¡ç†å™¨"""
        self.ai_dir = Path(__file__).parent
        # ä¿®æ”¹æ—¥å¿—ç›®å½•åˆ° articles ä¸‹
        self.logs_dir = Path(__file__).parent.parent.parent / 'articles' / 'analysis_records'
        self.cache_dir = self.ai_dir / 'cache'
        
        # èŽ·å–æ ¹ç›®å½•çš„ config.json è·¯å¾„
        self.config_path = Path(__file__).parent.parent.parent / 'config.json'
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.logs_dir.mkdir(exist_ok=True, parents=True)
        self.cache_dir.mkdir(exist_ok=True)
        
        # åˆ†æžè®°å½•ç´¢å¼•æ–‡ä»¶
        self.index_file = self.logs_dir / 'analysis_index.json'
        self._initialize_index()
        
        logger.info(f"âœ… åˆ†æžè®°å½•ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   æ—¥å¿—ç›®å½•: {self.logs_dir}")
        logger.info(f"   ç¼“å­˜ç›®å½•: {self.cache_dir}")
        logger.info(f"   é…ç½®æ–‡ä»¶: {self.config_path}")
    
    def _initialize_index(self) -> None:
        """åˆå§‹åŒ–åˆ†æžç´¢å¼•æ–‡ä»¶"""
        if not self.index_file.exists():
            index_data = {
                'version': '1.0.0',
                'created_at': datetime.now().isoformat(),
                'last_updated': datetime.now().isoformat(),
                'total_analyses': 0,
                'config_md5': self._calculate_config_md5(),
                'analyses': {}  # æ”¹ä¸ºå­—å…¸ï¼Œkeyä¸ºæºæ–‡ä»¶åï¼Œå€¼ä¸ºåˆ†æžè®°å½•
            }
            self._save_json(self.index_file, index_data)
            logger.info(f"âœ… åˆ›å»ºåˆ†æžç´¢å¼•æ–‡ä»¶: {self.index_file}")
    
    def _calculate_config_md5(self) -> str:
        """
        è®¡ç®— config.json çš„ MD5 æ ¡éªŒå’Œ
        ç”¨äºŽæ£€æµ‹é…ç½®å˜åŒ–ï¼Œè¯†åˆ«éœ€è¦é‡æ–°åˆ†æžçš„ç»“æžœ
        
        Returns:
            MD5 å“ˆå¸Œå€¼å­—ç¬¦ä¸²ï¼Œå¦‚æžœæ–‡ä»¶ä¸å­˜åœ¨è¿”å›žç©ºå­—ç¬¦ä¸²
        """
        if not self.config_path.exists():
            logger.warning(f"âš ï¸ config.json æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
            return ""
        
        try:
            with open(self.config_path, 'rb') as f:
                md5_hash = hashlib.md5()
                for chunk in iter(lambda: f.read(4096), b''):
                    md5_hash.update(chunk)
                return md5_hash.hexdigest()
        except Exception as e:
            logger.error(f"âŒ è®¡ç®— config.json MD5 å¤±è´¥: {str(e)}")
            return ""
    
    def _is_config_changed(self, stored_md5: str) -> bool:
        """
        æ£€æŸ¥é…ç½®æ˜¯å¦å·²å˜åŒ–
        
        Args:
            stored_md5: å­˜å‚¨çš„ MD5 å€¼
            
        Returns:
            True å¦‚æžœé…ç½®å·²å˜åŒ–ï¼ŒFalse è¡¨ç¤ºæœªå˜åŒ–
        """
        current_md5 = self._calculate_config_md5()
        changed = stored_md5 != current_md5
        
        if changed and stored_md5:
            logger.warning(
                f"âš ï¸ æ£€æµ‹åˆ°é…ç½®å˜åŒ–:\n"
                f"  æ—§ MD5: {stored_md5[:8]}...\n"
                f"  æ–° MD5: {current_md5[:8]}..."
            )
        
        return changed
    
    def record_analysis(self, 
                       user_profile: Dict[str, Any],
                       news_data: Dict[str, Any],
                       analysis_result: Dict[str, Any],
                       news_file_path: str = None) -> str:
        """
        è®°å½•ä¸€æ¬¡åˆ†æžç»“æžœ
        æŒ‰æºæ–‡ä»¶åç§°ä¿å­˜åˆ†æžè®°å½•ï¼Œé¿å…é‡å¤åˆ†æžç›¸åŒæ–‡ä»¶
        
        Args:
            user_profile: ç”¨æˆ·èµ„æ–™
            news_data: æ–°é—»æ•°æ®
            analysis_result: åˆ†æžç»“æžœ
            news_file_path: åŽŸå§‹æ–°é—»æ–‡ä»¶è·¯å¾„
            
        Returns:
            è®°å½•æ–‡ä»¶çš„è·¯å¾„
        """
        try:
            # ä»Žæºæ–‡ä»¶è·¯å¾„ç”Ÿæˆè®°å½•æ–‡ä»¶åï¼ˆä¸Žæºæ–‡ä»¶åç§°ç›¸åŒï¼‰
            if news_file_path:
                # æå–æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ï¼‰
                source_filename = os.path.basename(news_file_path)
                # å¦‚æžœä¸æ˜¯ .json æ‰©å±•åï¼Œåˆ™æ·»åŠ 
                if not source_filename.endswith('.json'):
                    filename = os.path.splitext(source_filename)[0] + '.json'
                else:
                    filename = source_filename
            else:
                # å¦‚æžœæ²¡æœ‰æºæ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨æ ‡é¢˜ç”Ÿæˆæ–‡ä»¶å
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                news_title = news_data.get('title', 'untitled')
                safe_title = ''.join(c if c.isalnum() or c in '-_' else '_' 
                                    for c in news_title[:30])
                filename = f"{timestamp}_{safe_title}.json"
            
            record_path = self.logs_dir / filename
            
            # è®¡ç®—å½“å‰é…ç½®çš„ MD5
            config_md5 = self._calculate_config_md5()
            
            # æž„å»ºå®Œæ•´è®°å½•ï¼ˆåŒ…å«é…ç½®æ ¡éªŒä¿¡æ¯ï¼‰
            record = {
                'timestamp': datetime.now().isoformat(),
                'status': 'recorded',
                'config_md5': config_md5,  # è®°å½•é…ç½®çš„ MD5 å“ˆå¸Œå€¼
                'user_profile': user_profile,
                'news_input': {
                    'title': news_data.get('title'),
                    'content_length': len(news_data.get('content', '')),
                    'source': news_data.get('source'),
                    'publish_date': news_data.get('publish_date'),
                    'file_path': news_file_path
                },
                'analysis_output': analysis_result
            }
            
            # ä¿å­˜è®°å½•
            self._save_json(record_path, record)
            
            # æ›´æ–°ç´¢å¼•
            self._update_index(record, filename)
            
            logger.info(f"âœ… åˆ†æžç»“æžœå·²è®°å½•: {record_path}")
            return str(record_path)
        
        except Exception as e:
            logger.error(f"âŒ è®°å½•åˆ†æžç»“æžœå¤±è´¥: {str(e)}")
            raise
    
    def _update_index(self, record: Dict[str, Any], filename: str) -> None:
        """æ›´æ–°åˆ†æžç´¢å¼•"""
        try:
            index = self._load_json(self.index_file)
            
            # ä½¿ç”¨æ–‡ä»¶åä½œä¸º keyï¼Œå­˜å‚¨åˆ†æžè®°å½•ä¿¡æ¯
            index['analyses'][filename] = {
                'filename': filename,
                'timestamp': record['timestamp'],
                'news_title': record['news_input'].get('title', 'untitled'),
                'relevance_score': record['analysis_output'].get('relevance_score'),
                'config_md5': record['config_md5']  # è®°å½•é…ç½® MD5 ç”¨äºŽæ£€æµ‹è¿‡æœŸ
            }
            
            index['total_analyses'] = len(index['analyses'])
            index['last_updated'] = datetime.now().isoformat()
            
            self._save_json(self.index_file, index)
            logger.info(f"âœ… åˆ†æžç´¢å¼•å·²æ›´æ–°: æ€»æ•° {index['total_analyses']}")
        
        except Exception as e:
            logger.warning(f"âš ï¸ æ›´æ–°ç´¢å¼•å¤±è´¥: {str(e)}")
    
    def get_analysis_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        èŽ·å–åˆ†æžåŽ†å²è®°å½•
        
        Args:
            limit: è¿”å›žçš„æœ€å¤§è®°å½•æ•°
            
        Returns:
            åˆ†æžåŽ†å²åˆ—è¡¨
        """
        try:
            index = self._load_json(self.index_file)
            
            # å°†å­—å…¸è½¬æ¢ä¸ºåˆ—è¡¨ï¼ŒæŒ‰æ—¶é—´æˆ³æŽ’åº
            analyses = index.get('analyses', {})
            if isinstance(analyses, dict):
                # è½¬æ¢å­—å…¸ä¸ºåˆ—è¡¨
                analyses_list = list(analyses.values())
            else:
                # å¦‚æžœæ˜¯åˆ—è¡¨ï¼ˆå‘åŽå…¼å®¹ï¼‰ï¼Œç›´æŽ¥ä½¿ç”¨
                analyses_list = analyses
            
            # æŒ‰æ—¶é—´æˆ³æŽ’åºï¼Œè¿”å›žæœ€æ–°çš„ limit æ¡è®°å½•
            sorted_analyses = sorted(analyses_list, key=lambda x: x.get('timestamp', ''), reverse=True)
            return sorted_analyses[:limit]
        
        except Exception as e:
            logger.error(f"âŒ èŽ·å–åˆ†æžåŽ†å²å¤±è´¥: {str(e)}")
            return []
    
    def has_analysis(self, filename: str) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å®Œæ•´åˆ†æžè¿‡
        å¿…é¡»åŒæ—¶æ»¡è¶³ï¼šç´¢å¼•ä¸­æœ‰è®°å½• AND å¯¹åº”çš„ç‰©ç†æ–‡ä»¶å­˜åœ¨
        å¦‚æžœä»»ä¸€ç¼ºå¤±ï¼Œåˆ™è®¤ä¸ºéœ€è¦é‡æ–°åˆ†æž
        
        Args:
            filename: æºæ–‡ä»¶å
            
        Returns:
            True è¡¨ç¤ºå·²å®Œæ•´åˆ†æžï¼ˆç´¢å¼•+æ–‡ä»¶éƒ½å­˜åœ¨ï¼‰ï¼ŒFalse è¡¨ç¤ºéœ€è¦åˆ†æž
        """
        try:
            # ç¡®ä¿æ–‡ä»¶åæ ¼å¼ä¸€è‡´
            if not filename.endswith('.json'):
                filename = os.path.splitext(filename)[0] + '.json'
            
            # 1. æ£€æŸ¥ç´¢å¼•ä¸­æ˜¯å¦æœ‰è®°å½•
            index = self._load_json(self.index_file)
            analyses = index.get('analyses', {})
            
            has_index = False
            if isinstance(analyses, dict):
                has_index = filename in analyses
            else:
                has_index = any(a.get('filename') == filename for a in analyses)
            
            logger.debug(f"ðŸ“‹ ç´¢å¼•æ£€æŸ¥: {filename} - {'âœ… æœ‰è®°å½•' if has_index else 'âŒ æ— è®°å½•'}")
            
            # 2. æ£€æŸ¥ç‰©ç†æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            record_file_path = self.logs_dir / filename
            file_exists = record_file_path.exists()
            
            logger.debug(f"ðŸ“ æ–‡ä»¶æ£€æŸ¥: {filename} - {'âœ… æ–‡ä»¶å­˜åœ¨' if file_exists else 'âŒ æ–‡ä»¶ä¸å­˜åœ¨'}")
            logger.debug(f"   è·¯å¾„: {record_file_path}")
            
            # 3. åªæœ‰ç´¢å¼•å’Œæ–‡ä»¶éƒ½å­˜åœ¨æ—¶æ‰è®¤ä¸ºå·²åˆ†æž
            both_exist = has_index and file_exists
            
            if has_index and not file_exists:
                logger.warning(f"âš ï¸ ç´¢å¼•è®°å½•å­˜åœ¨ä½†æ–‡ä»¶ç¼ºå¤±: {filename}")
                logger.info(f"ðŸ”„ éœ€è¦é‡æ–°åˆ†æžï¼ˆæ–‡ä»¶å·²åˆ é™¤ï¼‰")
            elif file_exists and not has_index:
                logger.warning(f"âš ï¸ ç‰©ç†æ–‡ä»¶å­˜åœ¨ä½†ç´¢å¼•è®°å½•ç¼ºå¤±: {filename}")
                logger.info(f"ðŸ”„ éœ€è¦é‡æ–°åˆ†æžï¼ˆç´¢å¼•ä¸åŒæ­¥ï¼‰")
            elif both_exist:
                logger.info(f"âœ… æ–‡ä»¶å®Œæ•´åˆ†æžè¿‡: {filename}")
            else:
                logger.debug(f"âŒ æ–‡ä»¶æœªåˆ†æž: {filename}")
            
            return both_exist
        
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥åˆ†æžçŠ¶æ€å¤±è´¥: {str(e)}")
            return False
    
    def get_analysis_record(self, filename: str) -> Optional[Dict[str, Any]]:
        """
        èŽ·å–æ–‡ä»¶çš„åˆ†æžè®°å½•
        
        Args:
            filename: æºæ–‡ä»¶å
            
        Returns:
            åˆ†æžè®°å½•å­—å…¸ï¼Œä¸å­˜åœ¨è¿”å›ž None
        """
        try:
            # å…ˆæ£€æŸ¥ç´¢å¼•
            index = self._load_json(self.index_file)
            analyses = index.get('analyses', {})
            
            if isinstance(analyses, dict):
                if filename in analyses:
                    # è¿”å›žç´¢å¼•ä¸­çš„è®°å½•ä¿¡æ¯
                    return analyses[filename]
            else:
                # åˆ—è¡¨ç»“æž„æ”¯æŒï¼ˆå‘åŽå…¼å®¹ï¼‰
                for a in analyses:
                    if a.get('filename') == filename:
                        return a
            
            logger.warning(f"âš ï¸ æœªåœ¨ç´¢å¼•ä¸­æ‰¾åˆ°: {filename}")
            return None
        
        except Exception as e:
            logger.error(f"âŒ èŽ·å–åˆ†æžè®°å½•å¤±è´¥: {str(e)}")
            return None
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        èŽ·å–åˆ†æžç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            index = self._load_json(self.index_file)
            analyses = index.get('analyses', {})
            
            # è½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆå¦‚æžœæ˜¯å­—å…¸ï¼‰
            if isinstance(analyses, dict):
                analyses_list = list(analyses.values())
            else:
                analyses_list = analyses
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'total_analyses': index.get('total_analyses', 0),
                'average_relevance_score': 0,
                'score_distribution': {
                    'high': 0,      # 8-10
                    'medium': 0,    # 5-7
                    'low': 0        # 0-4
                }
            }
            
            scores = []
            for analysis in analyses_list:
                score = analysis.get('relevance_score')
                if score is not None:
                    scores.append(score)
                    
                    if score >= 8:
                        stats['score_distribution']['high'] += 1
                    elif score >= 5:
                        stats['score_distribution']['medium'] += 1
                    else:
                        stats['score_distribution']['low'] += 1
            
            if scores:
                stats['average_relevance_score'] = sum(scores) / len(scores)
            
            return stats
        
        except Exception as e:
            logger.error(f"âŒ èŽ·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}
    
    def load_analysis(self, filename: str) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½å·²ä¿å­˜çš„åˆ†æžè®°å½•
        æ£€æµ‹é…ç½®æ˜¯å¦å·²å˜åŒ–ï¼Œé¿å…ä½¿ç”¨è¿‡æœŸçš„åˆ†æžç»“æžœ
        
        Args:
            filename: è®°å½•æ–‡ä»¶å
            
        Returns:
            åˆ†æžè®°å½•å­—å…¸ï¼Œä¸å­˜åœ¨æˆ–é…ç½®å·²å˜åŒ–è¿”å›ž None
        """
        try:
            record_path = self.logs_dir / filename
            
            if not record_path.exists():
                logger.warning(f"âš ï¸ åˆ†æžè®°å½•ä¸å­˜åœ¨: {filename}")
                return None
            
            record = self._load_json(record_path)
            
            # æ£€æŸ¥é…ç½®æ˜¯å¦å·²å˜åŒ–
            stored_md5 = record.get('config_md5', '')
            if self._is_config_changed(stored_md5):
                logger.warning(
                    f"âš ï¸ åˆ†æžè®°å½•å¯èƒ½å·²è¿‡æœŸï¼ˆé…ç½®å·²å˜ï¼‰: {filename}\n"
                    f"   å»ºè®®é‡æ–°åˆ†æž"
                )
                return None
            
            logger.info(f"âœ… å·²åŠ è½½åˆ†æžè®°å½•: {filename}")
            return record
        
        except Exception as e:
            logger.error(f"âŒ åŠ è½½åˆ†æžè®°å½•å¤±è´¥: {str(e)}")
            return None
    
    def check_analysis_validity(self, filename: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥åˆ†æžç»“æžœçš„æœ‰æ•ˆæ€§
        éªŒè¯æ–‡ä»¶å­˜åœ¨æ€§å’Œé…ç½®ä¸€è‡´æ€§
        
        Args:
            filename: è®°å½•æ–‡ä»¶å
            
        Returns:
            åŒ…å«æœ‰æ•ˆæ€§æ£€æŸ¥ç»“æžœçš„å­—å…¸
        """
        result = {
            'filename': filename,
            'exists': False,
            'config_valid': False,
            'needs_reanalysis': False,
            'details': ''
        }
        
        try:
            record_path = self.logs_dir / filename
            
            # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
            if not record_path.exists():
                result['details'] = 'åˆ†æžè®°å½•æ–‡ä»¶ä¸å­˜åœ¨'
                result['needs_reanalysis'] = True
                return result
            
            result['exists'] = True
            
            # æ£€æŸ¥é…ç½®ä¸€è‡´æ€§
            record = self._load_json(record_path)
            stored_md5 = record.get('config_md5', '')
            
            if not self._is_config_changed(stored_md5):
                result['config_valid'] = True
                result['details'] = 'åˆ†æžç»“æžœæœ‰æ•ˆ'
            else:
                result['config_valid'] = False
                result['needs_reanalysis'] = True
                result['details'] = f'é…ç½®å·²å˜åŒ– (æ—§MD5: {stored_md5[:8]}...)'
            
            return result
        
        except Exception as e:
            result['details'] = f'æ£€æŸ¥å¼‚å¸¸: {str(e)}'
            result['needs_reanalysis'] = True
            logger.error(f"âŒ æ£€æŸ¥åˆ†æžæœ‰æ•ˆæ€§å¤±è´¥: {str(e)}")
            return result
    
    def cache_analysis(self, cache_key: str, data: Dict[str, Any]) -> None:
        """
        ç¼“å­˜åˆ†æžç»“æžœä»¥é¿å…é‡å¤å¤„ç†
        
        Args:
            cache_key: ç¼“å­˜é”®ï¼ˆé€šå¸¸ä¸ºæ–°é—»æ ‡é¢˜çš„å“ˆå¸Œå€¼ï¼‰
            data: è¦ç¼“å­˜çš„æ•°æ®
        """
        try:
            cache_path = self.cache_dir / f"{cache_key}.json"
            
            cache_entry = {
                'cache_key': cache_key,
                'cached_at': datetime.now().isoformat(),
                'config_md5': self._calculate_config_md5(),
                'data': data
            }
            
            self._save_json(cache_path, cache_entry)
            logger.info(f"âœ… åˆ†æžç»“æžœå·²ç¼“å­˜: {cache_key}")
        
        except Exception as e:
            logger.warning(f"âš ï¸ ç¼“å­˜åˆ†æžç»“æžœå¤±è´¥: {str(e)}")
    
    def get_cached_analysis(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """
        èŽ·å–ç¼“å­˜çš„åˆ†æžç»“æžœ
        éªŒè¯é…ç½®ä¸€è‡´æ€§ï¼Œç¡®ä¿ç¼“å­˜ç»“æžœä»ç„¶æœ‰æ•ˆ
        
        Args:
            cache_key: ç¼“å­˜é”®
            
        Returns:
            ç¼“å­˜çš„æ•°æ®ï¼Œä¸å­˜åœ¨æˆ–é…ç½®å·²å˜åŒ–è¿”å›ž None
        """
        try:
            cache_path = self.cache_dir / f"{cache_key}.json"
            
            if not cache_path.exists():
                return None
            
            cache_entry = self._load_json(cache_path)
            
            # æ£€æŸ¥é…ç½®ä¸€è‡´æ€§
            stored_md5 = cache_entry.get('config_md5', '')
            if self._is_config_changed(stored_md5):
                logger.warning(f"âš ï¸ ç¼“å­˜å·²è¿‡æœŸï¼ˆé…ç½®å·²å˜ï¼‰: {cache_key}")
                return None
            
            return cache_entry.get('data')
        
        except Exception as e:
            logger.warning(f"âš ï¸ èŽ·å–ç¼“å­˜å¤±è´¥: {str(e)}")
            return None
    
    def find_outdated_analyses(self) -> List[Dict[str, Any]]:
        """
        æŸ¥æ‰¾æ‰€æœ‰é…ç½®å·²å˜åŒ–çš„åˆ†æžç»“æžœï¼ˆéœ€è¦é‡æ–°åˆ†æžï¼‰
        
        Returns:
            éœ€è¦é‡æ–°åˆ†æžçš„åˆ†æžè®°å½•åˆ—è¡¨
        """
        try:
            index = self._load_json(self.index_file)
            current_md5 = self._calculate_config_md5()
            analyses = index.get('analyses', {})
            
            # è½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆå¦‚æžœæ˜¯å­—å…¸ï¼‰
            if isinstance(analyses, dict):
                analyses_list = list(analyses.values())
            else:
                analyses_list = analyses
            
            outdated = []
            
            for analysis in analyses_list:
                filename = analysis.get('filename')
                stored_md5 = analysis.get('config_md5', '')
                
                if stored_md5 and stored_md5 != current_md5:
                    outdated.append({
                        'filename': filename,
                        'needs_reanalysis': True,
                        'reason': f'é…ç½®å·²å˜åŒ– (æ—§MD5: {stored_md5[:8]}...)'
                    })
            
            if outdated:
                logger.warning(f"âš ï¸ å‘çŽ° {len(outdated)} ä¸ªéœ€è¦é‡æ–°åˆ†æžçš„è®°å½•")
            
            return outdated
        
        except Exception as e:
            logger.error(f"âŒ æŸ¥æ‰¾è¿‡æœŸè®°å½•å¤±è´¥: {str(e)}")
            return []
    
    def export_to_csv(self, output_path: str = None) -> str:
        """
        å°†åˆ†æžç»“æžœå¯¼å‡ºä¸º CSV æ ¼å¼
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è¾“å‡ºæ–‡ä»¶çš„è·¯å¾„
        """
        try:
            if not output_path:
                output_path = str(self.logs_dir / f"analysis_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
            
            import csv
            
            index = self._load_json(self.index_file)
            analyses = index.get('analyses', {})
            
            # è½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆå¦‚æžœæ˜¯å­—å…¸ï¼‰
            if isinstance(analyses, dict):
                analyses_list = list(analyses.values())
            else:
                analyses_list = analyses
            
            with open(output_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=['timestamp', 'news_title', 'relevance_score', 'filename'])
                writer.writeheader()
                
                for analysis in analyses_list:
                    writer.writerow({
                        'timestamp': analysis.get('timestamp'),
                        'news_title': analysis.get('news_title'),
                        'relevance_score': analysis.get('relevance_score'),
                        'filename': analysis.get('filename')
                    })
            
            logger.info(f"âœ… åˆ†æžç»“æžœå·²å¯¼å‡º: {output_path}")
            return output_path
        
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºåˆ†æžç»“æžœå¤±è´¥: {str(e)}")
            raise
    
    def _save_json(self, filepath: Path, data: Dict[str, Any]) -> None:
        """ä¿å­˜ JSON æ–‡ä»¶"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _load_json(self, filepath: Path) -> Dict[str, Any]:
        """åŠ è½½ JSON æ–‡ä»¶"""
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
