#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI ç»“æ„åŒ–è¾“å‡ºæ¨¡å— - å­¦æ ¡é€šçŸ¥ç›¸å…³æ€§åˆ†æ
ä½¿ç”¨ Gemini API çš„ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½åˆ†ææ–‡ç« ä¸ç”¨æˆ·çš„ç›¸å…³ç¨‹åº¦
"""

import json
import os
import sys
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
from google import genai
from pydantic import BaseModel, Field

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import get_config
from logger_config import get_logger

logger = get_logger(__name__)


# å®šä¹‰ç»“æ„åŒ–è¾“å‡ºçš„æ•°æ®æ¨¡å‹
class ArticleRelevanceResponse(BaseModel):
    """æ–‡ç« ç›¸å…³æ€§åˆ†æçš„ç»“æ„åŒ–å“åº”æ¨¡å‹"""
    
    title: str = Field(description="æ–‡ç« çš„æ ‡é¢˜")
    summary: str = Field(description="æ–‡ç« å†…å®¹çš„ç®€æ˜æ€»ç»“ï¼Œæå–æ ¸å¿ƒä¿¡æ¯")
    relevance_score: float = Field(
        description="ä¸ç”¨æˆ·ç›¸å…³ç¨‹åº¦è¯„åˆ†ï¼ŒèŒƒå›´ä» 0 åˆ° 10ï¼Œ10 è¡¨ç¤ºä¸ç”¨æˆ·æœ€ç›¸å…³",
        ge=0,
        le=10
    )
    relevance_reason: str = Field(
        description="è¯„åˆ†åŸå› è¯´æ˜ï¼Œç®€è¿°ä¸ºä»€ä¹ˆè¿™æ¡æ–°é—»ä¸ç”¨æˆ·ç›¸å…³æˆ–ä¸ç›¸å…³"
    )


class StructuredAISummarizer:
    """ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºçš„ AI ç›¸å…³æ€§åˆ†æå™¨"""
    
    def __init__(self, config=None):
        """
        åˆå§‹åŒ–ç»“æ„åŒ– AI åˆ†æå™¨
        
        Args:
            config: é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or get_config()
        self._validate_config()
        self._initialize_storage()
        self._initialize_genai()
    
    def _initialize_storage(self) -> None:
        """åˆå§‹åŒ–æŒä¹…åŒ–å­˜å‚¨ç›®å½•"""
        self.articles_dir = os.path.join(
            os.path.dirname(os.path.dirname(__file__)),
            'articles'
        )
        self.analysis_dir = os.path.join(self.articles_dir, 'analysis_results')
        
        # åˆ›å»ºåˆ†æç»“æœç›®å½•
        os.makedirs(self.analysis_dir, exist_ok=True)
        logger.info(f"âœ… åˆ†æç»“æœå­˜å‚¨ç›®å½•: {self.analysis_dir}")
    
    def _validate_config(self) -> None:
        """éªŒè¯å¿…éœ€çš„é…ç½®é¡¹æ˜¯å¦å­˜åœ¨"""
        try:
            # éªŒè¯å¿…éœ€çš„æç¤ºè¯é…ç½®
            self.config.get_prompt('analyze_relevance')
            logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡ï¼šæ‰€æœ‰å¿…éœ€çš„æç¤ºè¯éƒ½å·²é…ç½®")
        except KeyError as e:
            logger.error(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {str(e)}")
            raise
    
    def _initialize_genai(self) -> None:
        """åˆå§‹åŒ– Gemini API"""
        api_key = self.config.gemini_api_key
        
        if not api_key:
            raise ValueError(
                "âŒ æœªè®¾ç½® Gemini API Keyï¼Œè¯·åœ¨ config.json ä¸­é…ç½® gemini.api_key"
            )
        
        # å­˜å‚¨ API Keyï¼Œåœ¨åˆ›å»º client æ—¶ä½¿ç”¨
        self.api_key = api_key
        
        # å¦‚æœå¯ç”¨äº†ä»£ç†ï¼Œé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®ä»£ç†
        # Gemini SDK ä¼šè‡ªåŠ¨è¯»å–è¿™äº›ç¯å¢ƒå˜é‡
        if self.config.proxy_enabled:
            proxy_url = self.config.get_proxy_url()
            if proxy_url:
                import os
                logger.info(f"ğŸ” ä»£ç†å·²å¯ç”¨: {self.config.get('proxy.host')}:{self.config.get('proxy.port')}")
                # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ŒGemini SDK ä¼šè‡ªåŠ¨ä½¿ç”¨
                os.environ['http_proxy'] = proxy_url
                os.environ['https_proxy'] = proxy_url
                # å¯¹äº requests åº“ä¹Ÿè®¾ç½®è¿™äº›
                os.environ['HTTP_PROXY'] = proxy_url
                os.environ['HTTPS_PROXY'] = proxy_url
                logger.info("âœ… ç¯å¢ƒå˜é‡ä»£ç†é…ç½®æˆåŠŸ")
        
        logger.info("âœ… Gemini API åˆå§‹åŒ–æˆåŠŸ")
        self.model_name = self.config.gemini_model
    
    def _load_response_schema(self) -> Dict[str, Any]:
        """
        åŠ è½½å“åº” Schema
        
        Returns:
            Schema å­—å…¸
        """
        schema_path = os.path.join(
            os.path.dirname(os.path.dirname(__file__)),
            'response_schema.json'
        )
        
        if not os.path.exists(schema_path):
            logger.warning(f"âš ï¸ å“åº” Schema æ–‡ä»¶ä¸å­˜åœ¨: {schema_path}")
            return self._get_default_schema()
        
        try:
            with open(schema_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Schema æ–‡ä»¶æ ¼å¼é”™è¯¯: {str(e)}")
            return self._get_default_schema()
    
    def _get_default_schema(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤çš„å“åº” Schema"""
        return {
            "type": "object",
            "properties": {
                "title": {
                    "type": "string",
                    "description": "æ–‡ç« çš„æ ‡é¢˜"
                },
                "summary": {
                    "type": "string",
                    "description": "æ–‡ç« å†…å®¹çš„ç®€æ˜æ€»ç»“ï¼Œæå–æ ¸å¿ƒä¿¡æ¯"
                },
                "relevance_score": {
                    "type": "number",
                    "description": "ä¸ç”¨æˆ·ç›¸å…³ç¨‹åº¦è¯„åˆ†ï¼ŒèŒƒå›´ä» 0 åˆ° 10ï¼Œ10 è¡¨ç¤ºä¸ç”¨æˆ·æœ€ç›¸å…³",
                    "minimum": 0,
                    "maximum": 10
                },
                "relevance_reason": {
                    "type": "string",
                    "description": "è¯„åˆ†åŸå› è¯´æ˜ï¼Œç®€è¿°ä¸ºä»€ä¹ˆè¿™æ¡æ–°é—»ä¸ç”¨æˆ·ç›¸å…³æˆ–ä¸ç›¸å…³"
                }
            },
            "required": ["title", "summary", "relevance_score", "relevance_reason"]
        }
    
    def analyze_article(self, article: Dict[str, Any], source_filename: str = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºåˆ†ææ–‡ç« ä¸ç”¨æˆ·çš„ç›¸å…³æ€§
        
        Args:
            article: æ–‡ç« æ•°æ®å­—å…¸
            source_filename: æºæ–‡ä»¶åï¼ˆç”¨äºä¿å­˜åˆ†æç»“æœï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
            
        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        try:
            content = article.get('content', '')
            original_title = article.get('title', '')
            
            if not content:
                logger.warning(f"âš ï¸ æ–‡ç« å†…å®¹ä¸ºç©º: {original_title}")
                return {
                    'status': 'error',
                    'message': 'æ–‡ç« å†…å®¹ä¸ºç©º',
                    'data': None,
                    'source_filename': source_filename,
                    'original_title': original_title
                }
            
            # è·å–ç”¨æˆ·ä¿¡æ¯å’Œæç¤ºè¯æ¨¡æ¿
            user_profile = self.config.get('user_profile', {})
            
            # ä»é…ç½®æ–‡ä»¶è·å–æç¤ºè¯æ¨¡æ¿ï¼ˆå¿…é¡»å­˜åœ¨ï¼‰
            try:
                prompt_template = self.config.get_prompt('analyze_relevance')
            except KeyError as e:
                logger.error(f"âŒ é…ç½®é”™è¯¯: {str(e)}")
                raise
            
            # æå–åµŒå¥—çš„ç”¨æˆ·ä¿¡æ¯
            basic_info = user_profile.get('basic_info', {})
            education = user_profile.get('education', {})
            interests = user_profile.get('interests', {})
            dislikes = user_profile.get('dislikes', {})
            
            # å¤„ç†å…´è¶£å’Œä¸æ„Ÿå…´è¶£çš„å†…å®¹ï¼ˆæ”¯æŒæ–°æ—§é…ç½®æ ¼å¼ï¼‰
            interests_topics = ', '.join(interests.get('topics', user_profile.get('interests', [])))
            interests_keywords = ', '.join(interests.get('keywords', user_profile.get('relevant_keywords', [])))
            dislike_topics = ', '.join(dislikes.get('topics', []))
            dislike_keywords = ', '.join(dislikes.get('keywords', []))
            
            # æ ¼å¼åŒ–æç¤ºè¯
            prompt = prompt_template.format(
                user_name=basic_info.get('name', user_profile.get('name', '')),
                student_id=basic_info.get('student_id', user_profile.get('student_id', '')),
                department=education.get('department', user_profile.get('department', '')),
                major=education.get('major', user_profile.get('major', '')),
                grade=education.get('grade', ''),
                **{'class': education.get('class', '')},  # ä½¿ç”¨ ** è§£åŒ…é¿å… class å…³é”®å­—
                interests=interests_topics,
                relevant_keywords=interests_keywords,
                uninterested_topics=dislike_topics,
                uninterested_keywords=dislike_keywords,
                title=original_title,
                content=content
            )
            
            logger.info(f"ğŸ”„ æ­£åœ¨åˆ†ææ–‡ç« ä¸ç”¨æˆ·çš„ç›¸å…³æ€§: {original_title[:50]}...")
            
            # è°ƒç”¨ Gemini API ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
            # æ–°ç‰ˆæœ¬ google-genai ä½¿ç”¨ç®€åŒ–çš„ Schema å®šä¹‰
            client = genai.Client(api_key=self.api_key)
            
            # å®šä¹‰å“åº” schema - æ–°ç‰ˆæœ¬ä½¿ç”¨ç®€åŒ–çš„å­—å…¸æ ¼å¼
            response_schema = {
                "type": "object",
                "properties": {
                    "title": {
                        "type": "string",
                        "description": "æ–‡ç« çš„æ ‡é¢˜"
                    },
                    "summary": {
                        "type": "string",
                        "description": "æ–‡ç« å†…å®¹çš„ç®€æ˜æ€»ç»“"
                    },
                    "relevance_score": {
                        "type": "number",
                        "description": "ä¸ç”¨æˆ·ç›¸å…³ç¨‹åº¦è¯„åˆ†ï¼ˆ0-10ï¼‰,10åˆ†è¡¨ç¤ºæœ€ç›¸å…³"
                    },
                    "relevance_reason": {
                        "type": "string",
                        "description": "è¯„åˆ†åŸå› è¯´æ˜ï¼Œç®€è¿°ä¸ºä»€ä¹ˆè¿™æ¡æ–°é—»ä¸ç”¨æˆ·ç›¸å…³æˆ–ä¸ç›¸å…³"
                    }
                },
                "required": ["title", "summary", "relevance_score", "relevance_reason"]
            }
            
            response = client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=genai.types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema=response_schema
                )
            )
            
            # è§£æå“åº”
            if response.text:
                result_data = json.loads(response.text)
                logger.info(f"âœ… æ–‡ç« åˆ†ææˆåŠŸ: {original_title[:50]}...")
                
                return {
                    'status': 'success',
                    'original_title': original_title,
                    'source_filename': source_filename,
                    'data': result_data
                }
            else:
                logger.error("âŒ API è¿”å›ç©ºå“åº”")
                return {
                    'status': 'error',
                    'message': 'API è¿”å›ç©ºå“åº”',
                    'data': None,
                    'source_filename': source_filename,
                    'original_title': original_title
                }
        
        except json.JSONDecodeError as e:
            logger.error(f"âŒ å“åº” JSON è§£æå¤±è´¥: {str(e)}")
            return {
                'status': 'error',
                'message': f'JSON è§£æå¤±è´¥: {str(e)}',
                'data': None,
                'source_filename': source_filename,
                'original_title': original_title
            }
        
        except Exception as e:
            logger.error(f"âŒ æ–‡ç« åˆ†æå¤±è´¥: {str(e)}")
            return {
                'status': 'error',
                'message': str(e),
                'data': None,
                'source_filename': source_filename,
                'original_title': original_title
            }
    
    def analyze_articles(self, articles: List[Dict[str, Any]], source_filenames: List[str] = None) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡åˆ†æå¤šç¯‡æ–‡ç« 
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            source_filenames: å¯¹åº”çš„æºæ–‡ä»¶ååˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        results = []
        total = len(articles)
        
        # å¦‚æœæ²¡æœ‰æä¾›æ–‡ä»¶åï¼Œåˆ™ä¸ºæ¯ç¯‡æ–‡ç« ç”Ÿæˆä¸€ä¸ª
        if not source_filenames:
            source_filenames = [None] * total
        
        logger.info(f"ğŸš€ å¼€å§‹åˆ†æ {total} ç¯‡æ–‡ç« ")
        
        for idx, (article, filename) in enumerate(zip(articles, source_filenames), 1):
            logger.info(f"[{idx}/{total}] å¤„ç†ä¸­...")
            result = self.analyze_article(article, source_filename=filename)
            results.append(result)
            
            # å¦‚æœåˆ†ææˆåŠŸï¼Œä¿å­˜å•ä¸ªæ–‡ç« ç»“æœ
            if result['status'] == 'success' and 'original_title' in result:
                self._save_single_article_result(result)
        
        logger.info(f"âœ… åˆ†æå®Œæˆï¼Œå…±å¤„ç† {len(results)} ç¯‡æ–‡ç« ")
        
        return results
    
    def _calculate_config_md5(self) -> str:
        """
        è®¡ç®— config.json çš„ MD5 æ ¡éªŒå’Œ
        
        Returns:
            MD5 å“ˆå¸Œå€¼
        """
        config_path = os.path.join(
            os.path.dirname(os.path.dirname(__file__)),
            'config.json'
        )
        
        if not os.path.exists(config_path):
            logger.warning(f"âš ï¸ config.json æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return ""
        
        try:
            with open(config_path, 'rb') as f:
                md5_hash = hashlib.md5()
                for chunk in iter(lambda: f.read(4096), b''):
                    md5_hash.update(chunk)
                return md5_hash.hexdigest()
        except Exception as e:
            logger.error(f"âŒ è®¡ç®— config.json MD5 å¤±è´¥: {str(e)}")
            return ""
    
    def _save_single_article_result(self, result: Dict[str, Any]) -> None:
        """
        ä¿å­˜å•ä¸ªæ–‡ç« çš„åˆ†æç»“æœåˆ°æ–‡ä»¶
        
        Args:
            result: åˆ†æç»“æœå­—å…¸
        """
        try:
            if 'original_title' not in result or result['status'] != 'success':
                return
            
            # ä»åŸå§‹æ ‡é¢˜ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨ MD5 ç¡®ä¿å”¯ä¸€æ€§ï¼‰
            # å‡è®¾æ–‡ç« å·²ç»æœ‰äº†æºæ–‡ä»¶åï¼Œä» data ä¸­æå–
            article_data = result.get('data', {})
            source_filename = result.get('source_filename', None)
            
            if not source_filename:
                # å¦‚æœæ²¡æœ‰æºæ–‡ä»¶åï¼Œç”Ÿæˆä¸€ä¸ª
                title_hash = hashlib.md5(result['original_title'].encode()).hexdigest()
                source_filename = f"{title_hash}.json"
            
            # æ„å»ºå®Œæ•´çš„è¾“å‡ºè·¯å¾„
            output_path = os.path.join(self.analysis_dir, source_filename)
            
            # æ„å»ºåŒ…å«å…ƒæ•°æ®çš„ç»“æœ
            persistence_data = {
                'source_file': source_filename,
                'generated_at': datetime.now().isoformat(),
                'config_md5': self._calculate_config_md5(),
                'analysis_result': article_data
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(persistence_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… åˆ†æç»“æœå·²ä¿å­˜: {output_path}")
        
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {str(e)}")
    
    def save_results(self, results: List[Dict[str, Any]], output_file: str = None) -> None:
        """
        æ‰¹é‡ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        
        æ³¨æ„ï¼šå•ä¸ªæ–‡ç« çš„ç»“æœå·²åœ¨ analyze_article() æ—¶è‡ªåŠ¨ä¿å­˜åˆ° articles/analysis_results/ ç›®å½•
        
        Args:
            results: åˆ†æç»“æœåˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºç”Ÿæˆæ±‡æ€»æŠ¥å‘Šï¼‰
        """
        if not output_file:
            logger.info("ğŸ’¾ å•ä¸ªæ–‡ç« ç»“æœå·²è‡ªåŠ¨ä¿å­˜åˆ° articles/analysis_results/ ç›®å½•")
            return
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ±‡æ€»ç»“æœå¤±è´¥: {str(e)}")
    
    def load_saved_result(self, filename: str) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½å·²ä¿å­˜çš„åˆ†æç»“æœ
        
        Args:
            filename: æ–‡ä»¶åï¼ˆç›¸å¯¹äº analysis_results ç›®å½•ï¼‰
            
        Returns:
            åˆ†æç»“æœå­—å…¸ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ– config å·²å˜åŒ–åˆ™è¿”å› None
        """
        result_path = os.path.join(self.analysis_dir, filename)
        
        if not os.path.exists(result_path):
            logger.warning(f"âš ï¸ åˆ†æç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {result_path}")
            return None
        
        try:
            with open(result_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # éªŒè¯ config.json æ˜¯å¦å·²å˜åŒ–
            stored_md5 = data.get('config_md5', '')
            current_md5 = self._calculate_config_md5()
            
            if stored_md5 != current_md5:
                logger.warning(
                    f"âš ï¸ é…ç½®å·²å˜åŒ–ï¼Œç¼“å­˜ç»“æœå¯èƒ½å·²è¿‡æœŸ: {filename}\n"
                    f"  æ—§ MD5: {stored_md5}\n"
                    f"  æ–° MD5: {current_md5}"
                )
                return None
            
            logger.info(f"âœ… å·²åŠ è½½ç¼“å­˜ç»“æœ: {filename}")
            return data
        
        except json.JSONDecodeError as e:
            logger.error(f"âŒ ç»“æœæ–‡ä»¶æ ¼å¼é”™è¯¯: {str(e)}")
            return None
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ç»“æœå¤±è´¥: {str(e)}")
            return None
    
    def result_exists(self, filename: str) -> bool:
        """
        æ£€æŸ¥åˆ†æç»“æœæ˜¯å¦å·²å­˜åœ¨ä¸”æœ‰æ•ˆ
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            True å¦‚æœç»“æœå­˜åœ¨ä¸” config æœªå˜åŒ–ï¼Œå¦åˆ™ False
        """
        return self.load_saved_result(filename) is not None
    
    def analyze_articles_from_dir(self, skip_existing: bool = True) -> List[Dict[str, Any]]:
        """
        ä» articles ç›®å½•è¯»å–æ‰€æœ‰æ–‡ç« å¹¶è¿›è¡Œåˆ†æ
        
        Args:
            skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„åˆ†æç»“æœï¼ˆconfig æœªå˜åŒ–æ—¶ï¼‰
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        # è¯»å– articles ç›®å½•ä¸­çš„æ‰€æœ‰ JSON æ–‡ä»¶
        article_files = []
        for filename in os.listdir(self.articles_dir):
            if filename.endswith('.json') and filename != 'index.json':
                article_files.append(filename)
        
        if not article_files:
            logger.warning("âš ï¸ åœ¨ articles ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ")
            return []
        
        articles = []
        filenames = []
        
        for filename in article_files:
            filepath = os.path.join(self.articles_dir, filename)
            
            # æ£€æŸ¥æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„ç»“æœ
            if skip_existing and self.result_exists(filename):
                logger.info(f"â­ï¸ è·³è¿‡å·²åˆ†æçš„æ–‡ç« ï¼ˆé…ç½®æœªå˜ï¼‰: {filename}")
                continue
            
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    article = json.load(f)
                articles.append(article)
                filenames.append(filename)
            except json.JSONDecodeError as e:
                logger.error(f"âŒ æ–‡ç« æ–‡ä»¶æ ¼å¼é”™è¯¯: {filename} - {str(e)}")
            except Exception as e:
                logger.error(f"âŒ è¯»å–æ–‡ç« å¤±è´¥: {filename} - {str(e)}")
        
        if articles:
            logger.info(f"ğŸ“š åŠ è½½äº† {len(articles)} ç¯‡æ–‡ç« è¿›è¡Œåˆ†æ")
            return self.analyze_articles(articles, source_filenames=filenames)
        else:
            logger.info("âœ… æ‰€æœ‰æ–‡ç« éƒ½å·²åˆ†æè¿‡ï¼Œæ— éœ€é‡æ–°åˆ†æ")
            return []
    
    def get_analysis_stats(self) -> Dict[str, Any]:
        """
        è·å–åˆ†æç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        if not os.path.exists(self.analysis_dir):
            return {
                'total_analyzed': 0,
                'analysis_results_dir': self.analysis_dir
            }
        
        result_files = [f for f in os.listdir(self.analysis_dir) if f.endswith('.json')]
        
        stats = {
            'total_analyzed': len(result_files),
            'analysis_results_dir': self.analysis_dir,
            'files': []
        }
        
        for filename in result_files:
            filepath = os.path.join(self.analysis_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    stats['files'].append({
                        'filename': filename,
                        'generated_at': data.get('generated_at'),
                        'relevance_score': data.get('analysis_result', {}).get('relevance_score')
                    })
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•è¯»å–ç»Ÿè®¡ä¿¡æ¯: {filename} - {str(e)}")
        
        return stats


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå®Œæ•´çš„åˆ†æå·¥ä½œæµ"""
    try:
        print("\n" + "="*70)
        print("ğŸš€ å­¦æ ¡é€šçŸ¥ç›¸å…³æ€§åˆ†æç³»ç»Ÿ")
        print("="*70)
        
        # åˆå§‹åŒ–åˆ†æå™¨
        analyzer = StructuredAISummarizer()
        
        # æ–¹å¼ 1: ä» articles ç›®å½•è‡ªåŠ¨è¯»å–å¹¶åˆ†ææ‰€æœ‰æ–‡ç« 
        print("\nğŸ“š ä» articles ç›®å½•è¯»å–å¹¶åˆ†ææ‰€æœ‰æ–‡ç« ...")
        results = analyzer.analyze_articles_from_dir(skip_existing=True)
        
        if results:
            # æ˜¾ç¤ºåˆ†æç»“æœæ‘˜è¦
            successful = sum(1 for r in results if r['status'] == 'success')
            print(f"\nâœ… åˆ†æå®Œæˆ: {successful}/{len(results)} ç¯‡æ–‡ç« æˆåŠŸåˆ†æ")
            
            # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
            for result in results[:3]:
                if result['status'] == 'success':
                    data = result['data']
                    print(f"\n  ğŸ“„ {data['title'][:30]}...")
                    print(f"     ç›¸å…³æ€§è¯„åˆ†: {data['relevance_score']}/10")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š åˆ†æç»Ÿè®¡ä¿¡æ¯:")
        stats = analyzer.get_analysis_stats()
        print(f"   æ€»åˆ†ææ–‡ç« æ•°: {stats['total_analyzed']}")
        print(f"   å­˜å‚¨ç›®å½•: {stats['analysis_results_dir']}")
        
        if stats['files']:
            print(f"\n   ğŸ“ˆ ç›¸å…³æ€§è¯„åˆ†åˆ†å¸ƒ:")
            scores = [f['relevance_score'] for f in stats['files'] if f['relevance_score'] is not None]
            if scores:
                avg_score = sum(scores) / len(scores)
                max_score = max(scores)
                min_score = min(scores)
                print(f"      å¹³å‡: {avg_score:.1f}/10 | æœ€é«˜: {max_score}/10 | æœ€ä½: {min_score}/10")
        
        # æ¼”ç¤ºåŠ è½½ç¼“å­˜ç»“æœ
        if stats['files']:
            first_file = stats['files'][0]['filename']
            print(f"\nğŸ’¾ æ¼”ç¤ºåŠ è½½ç¼“å­˜ç»“æœ: {first_file}")
            cached = analyzer.load_saved_result(first_file)
            if cached:
                result = cached['analysis_result']
                print(f"   æ ‡é¢˜: {result['title']}")
                print(f"   æ‘˜è¦: {result['summary'][:50]}...")
                print(f"   ç›¸å…³æ€§: {result['relevance_score']}/10")
        
        print("\n" + "="*70)
        print("âœ¨ åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° articles/analysis_results/ ç›®å½•")
        print("="*70 + "\n")
    
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
