#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SZTU æ–°é—»çˆ¬è™« - å‘½ä»¤è¡Œç‰ˆæœ¬
"""

import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ scraper
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from scraper import load_articles, fetch_news_pages, fetch_news_pages_with_json, fetch_articles_with_details, get_all_articles_from_index, load_articles_index
from logger_config import get_logger

logger = get_logger(__name__)


def list_articles():
    """åˆ—å‡ºæ‰€æœ‰å·²çˆ¬å–çš„æ–‡ç« """
    articles = get_all_articles_from_index()
    if not articles:
        logger.info("ğŸ“­ æš‚æ— æ–‡ç« è®°å½•")
        return
    
    logger.info(f"ğŸ“° å·²çˆ¬å–æ–‡ç« åˆ—è¡¨ (å…± {len(articles)} ç¯‡)")
    for i, article in enumerate(articles, 1):
        logger.info(f"{i}. ã€{article.get('category', 'N/A')}ã€‘{article.get('title', 'N/A')}")
        logger.info(f"   éƒ¨é—¨: {article.get('department', 'N/A')} | æ—¶é—´: {article.get('publish_time', 'N/A')}")
        
        # æ˜¾ç¤ºURLé“¾æ¥
        url = article.get('url', '')
        if url:
            logger.info(f"   é“¾æ¥: {url}")
        
        filename = article.get('filename', '')
        if filename:
            logger.info(f"   æ–‡ä»¶: {filename}")


def fetch_news():
    """çˆ¬å–æ–°é—»å¹¶ä¿å­˜å®Œæ•´å†…å®¹"""
    while True:
        try:
            pages = int(input("è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µæ•° (1-10): "))
            if 1 <= pages <= 10:
                break
            logger.warning("âŒ é¡µæ•°å¿…é¡»åœ¨ 1-10 ä¹‹é—´")
        except ValueError:
            logger.warning("âŒ è¯·è¾“å…¥æ­£ç¡®çš„é¡µæ•°")
    
    fetch_articles_with_details(pages)


def fetch_news_json():
    """çˆ¬å–æ–°é—»æ ‡é¢˜å’Œé“¾æ¥ï¼Œä¿å­˜ä¸º JSON"""
    while True:
        try:
            pages = int(input("è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µæ•° (1-10): "))
            if 1 <= pages <= 10:
                break
            logger.warning("âŒ é¡µæ•°å¿…é¡»åœ¨ 1-10 ä¹‹é—´")
        except ValueError:
            logger.warning("âŒ è¯·è¾“å…¥æ­£ç¡®çš„é¡µæ•°")
    
    fetch_news_pages_with_json(pages)


def search_by_url():
    """æ ¹æ® URL æŸ¥è¯¢æ–‡ç« ä¿¡æ¯"""
    url = input("\nè¯·è¾“å…¥æ–‡ç«  URL: ").strip()
    
    if not url:
        logger.warning("âŒ URL ä¸èƒ½ä¸ºç©º")
        return
    
    index = load_articles_index()
    
    if url in index:
        article_info = index[url]
        logger.info("âœ… æ‰¾åˆ°æ–‡ç« ï¼")
        logger.info(f"æ ‡é¢˜: {article_info.get('title', 'N/A')}")
        logger.info(f"ç±»åˆ«: {article_info.get('category', 'N/A')}")
        logger.info(f"éƒ¨é—¨: {article_info.get('department', 'N/A')}")
        logger.info(f"å‘å¸ƒæ—¶é—´: {article_info.get('publish_time', 'N/A')}")
        logger.info(f"é™„ä»¶: {'æœ‰' if article_info.get('has_attachment') else 'æ— '}")
        logger.info(f"æ–‡ä»¶: {article_info.get('filename', 'N/A')}")
        logger.info(f"çˆ¬å–æ—¶é—´: {article_info.get('fetch_time', 'N/A')}")
    else:
        logger.warning(f"âŒ æœªæ‰¾åˆ°è¯¥ URL çš„æ–‡ç« ")


def search_by_title():
    """æ ¹æ®æ ‡é¢˜å…³é”®è¯æœç´¢æ–‡ç« """
    keyword = input("\nè¯·è¾“å…¥æ ‡é¢˜å…³é”®è¯: ").strip()
    
    if not keyword:
        logger.warning("âŒ å…³é”®è¯ä¸èƒ½ä¸ºç©º")
        return
    
    articles = get_all_articles_from_index()
    results = [a for a in articles if keyword.lower() in a.get('title', '').lower()]
    
    if results:
        logger.info(f"âœ… æ‰¾åˆ° {len(results)} ç¯‡ç›¸å…³æ–‡ç« ï¼š")
        for i, article in enumerate(results, 1):
            logger.info(f"{i}. {article.get('title', 'N/A')}")
            logger.info(f"   æ–‡ä»¶: {article.get('filename', 'N/A')}")
            logger.info(f"   éƒ¨é—¨: {article.get('department', 'N/A')}")
    else:
        logger.warning(f"âŒ æœªæ‰¾åˆ°åŒ…å« '{keyword}' çš„æ–‡ç« ")


def main():
    logger.info("=" * 40)
    logger.info("ğŸ“° SZTU æ–°é—»çˆ¬è™«")
    logger.info("=" * 40)
    
    while True:
        logger.info("\nè¯·é€‰æ‹©æ“ä½œ:")
        logger.info("1. çˆ¬å–æ–°é—»æ ‡é¢˜å’Œé“¾æ¥ï¼ˆä¿å­˜ä¸º JSONï¼‰")
        logger.info("2. çˆ¬å–å®Œæ•´æ–‡ç« ï¼ˆæ ‡é¢˜ã€å†…å®¹ã€æ—¶é—´ç­‰ï¼‰")
        logger.info("3. æŸ¥çœ‹å·²çˆ¬å–çš„æ–°é—»")
        logger.info("4. æ ¹æ® URL æŸ¥è¯¢æ–‡ç« ")
        logger.info("5. æ ¹æ®æ ‡é¢˜æœç´¢æ–‡ç« ")
        logger.info("6. å¯åŠ¨ Web æµè§ˆç•Œé¢")
        logger.info("7. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-7): ").strip()
        
        if choice == "1":
            fetch_news_json()
        elif choice == "2":
            fetch_news()
        elif choice == "3":
            list_articles()
        elif choice == "4":
            search_by_url()
        elif choice == "5":
            search_by_title()
        elif choice == "6":
            import subprocess
            import sys
            logger.info("ğŸš€ å¯åŠ¨ Streamlit Web åº”ç”¨...")
            logger.info("ğŸ“± è®¿é—®åœ°å€: http://localhost:8501")
            subprocess.run([
                sys.executable, "-m", "streamlit", "run",
                os.path.join(os.path.dirname(__file__), "streamlit_app.py")
            ])
        elif choice == "7":
            logger.info("ğŸ‘‹ å†è§ï¼")
            break
        else:
            logger.warning("âŒ æ— æ•ˆçš„é€‰é¡¹")


if __name__ == "__main__":
    main()
