#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SZTU æ–°é—»çˆ¬è™« - å‘½ä»¤è¡Œç‰ˆæœ¬
æ”¯æŒçˆ¬è™«å’Œ AI åˆ†æåŠŸèƒ½
"""

import sys
import os
import json

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from scraper import load_articles, fetch_news_pages, fetch_news_pages_with_json, fetch_articles_with_details, get_all_articles_from_index, load_articles_index
from logger_config import get_logger
from ai.dify_workflow import DifyWorkflowHandler
from ai.analysis_recorder import AnalysisRecorder
from config import get_config

logger = get_logger(__name__)


def list_articles():
    """åˆ—å‡ºæ‰€æœ‰å·²çˆ¬å–çš„æ–‡ç« """
    articles = get_all_articles_from_index()
    if not articles:
        logger.info("ğŸ“­ æš‚æ— æ–‡ç« è®°å½•")
        return
    
    logger.info(f"ğŸ“° å·²çˆ¬å–æ–‡ç« åˆ—è¡¨ (å…± {len(articles)} ç¯‡)")
    for i, article in enumerate(articles, 1):
        logger.info(f"{i}. ã€{article.get('category', 'N/A')}ã€‘{article.get('title', 'N/A')}")
        logger.info(f"   éƒ¨é—¨: {article.get('department', 'N/A')} | æ—¶é—´: {article.get('publish_time', 'N/A')}")
        
        # æ˜¾ç¤ºURLé“¾æ¥
        url = article.get('url', '')
        if url:
            logger.info(f"   é“¾æ¥: {url}")
        
        filename = article.get('filename', '')
        if filename:
            logger.info(f"   æ–‡ä»¶: {filename}")


def fetch_news():
    """çˆ¬å–æ–°é—»å¹¶ä¿å­˜å®Œæ•´å†…å®¹"""
    while True:
        try:
            pages = int(input("è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µæ•° (1-10): "))
            if 1 <= pages <= 10:
                break
            logger.warning("âŒ é¡µæ•°å¿…é¡»åœ¨ 1-10 ä¹‹é—´")
        except ValueError:
            logger.warning("âŒ è¯·è¾“å…¥æ­£ç¡®çš„é¡µæ•°")
    
    fetch_articles_with_details(pages)


def fetch_news_json():
    """çˆ¬å–æ–°é—»æ ‡é¢˜å’Œé“¾æ¥ï¼Œä¿å­˜ä¸º JSON"""
    while True:
        try:
            pages = int(input("è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µæ•° (1-10): "))
            if 1 <= pages <= 10:
                break
            logger.warning("âŒ é¡µæ•°å¿…é¡»åœ¨ 1-10 ä¹‹é—´")
        except ValueError:
            logger.warning("âŒ è¯·è¾“å…¥æ­£ç¡®çš„é¡µæ•°")
    
    fetch_news_pages_with_json(pages)


def search_by_url():
    """æ ¹æ® URL æŸ¥è¯¢æ–‡ç« ä¿¡æ¯"""
    url = input("\nè¯·è¾“å…¥æ–‡ç«  URL: ").strip()
    
    if not url:
        logger.warning("âŒ URL ä¸èƒ½ä¸ºç©º")
        return
    
    index = load_articles_index()
    
    if url in index:
        article_info = index[url]
        logger.info("âœ… æ‰¾åˆ°æ–‡ç« ï¼")
        logger.info(f"æ ‡é¢˜: {article_info.get('title', 'N/A')}")
        logger.info(f"ç±»åˆ«: {article_info.get('category', 'N/A')}")
        logger.info(f"éƒ¨é—¨: {article_info.get('department', 'N/A')}")
        logger.info(f"å‘å¸ƒæ—¶é—´: {article_info.get('publish_time', 'N/A')}")
        logger.info(f"é™„ä»¶: {'æœ‰' if article_info.get('has_attachment') else 'æ— '}")
        logger.info(f"æ–‡ä»¶: {article_info.get('filename', 'N/A')}")
        logger.info(f"çˆ¬å–æ—¶é—´: {article_info.get('fetch_time', 'N/A')}")
    else:
        logger.warning(f"âŒ æœªæ‰¾åˆ°è¯¥ URL çš„æ–‡ç« ")


def search_by_title():
    """æ ¹æ®æ ‡é¢˜å…³é”®è¯æœç´¢æ–‡ç« """
    keyword = input("\nè¯·è¾“å…¥æ ‡é¢˜å…³é”®è¯: ").strip()
    
    if not keyword:
        logger.warning("âŒ å…³é”®è¯ä¸èƒ½ä¸ºç©º")
        return
    
    articles = get_all_articles_from_index()
    results = [a for a in articles if keyword.lower() in a.get('title', '').lower()]
    
    if results:
        logger.info(f"âœ… æ‰¾åˆ° {len(results)} ç¯‡ç›¸å…³æ–‡ç« ï¼š")
        for i, article in enumerate(results, 1):
            logger.info(f"{i}. {article.get('title', 'N/A')}")
            logger.info(f"   æ–‡ä»¶: {article.get('filename', 'N/A')}")
            logger.info(f"   éƒ¨é—¨: {article.get('department', 'N/A')}")
    else:
        logger.warning(f"âŒ æœªæ‰¾åˆ°åŒ…å« '{keyword}' çš„æ–‡ç« ")


def analyze_news_with_ai():
    """ä½¿ç”¨ AI åˆ†ææ–°é—»çš„ç›¸å…³æ€§"""
    try:
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ¤– AI æ–°é—»ç›¸å…³æ€§åˆ†æ")
        logger.info("=" * 70)
        
        # åˆå§‹åŒ–å¤„ç†å™¨å’Œè®°å½•å™¨
        handler = DifyWorkflowHandler()
        recorder = AnalysisRecorder()
        config = get_config()
        
        logger.info("\nğŸ“‹ é€‰æ‹©åˆ†ææ–¹å¼:")
        logger.info("1. åˆ†æå•ç¯‡æ–‡ç« ")
        logger.info("2. æ‰¹é‡åˆ†ææ‰€æœ‰æ–‡ç« ")
        logger.info("3. æŸ¥çœ‹åˆ†æå†å²")
        logger.info("4. æ£€æŸ¥åˆ†æç»“æœæœ‰æ•ˆæ€§")
        logger.info("5. è¿”å›ä¸»èœå•")
        
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-5): ").strip()
        
        if choice == "1":
            _analyze_single_article(handler, recorder, config)
        elif choice == "2":
            _analyze_all_articles(handler, recorder, config)
        elif choice == "3":
            _view_analysis_history(recorder)
        elif choice == "4":
            _check_analysis_validity(recorder)
        elif choice == "5":
            return
        else:
            logger.warning("âŒ æ— æ•ˆçš„é€‰é¡¹")
    
    except Exception as e:
        logger.error(f"âŒ AI åˆ†æå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


def _analyze_single_article(handler, recorder, config):
    """åˆ†æå•ç¯‡æ–‡ç« """
    articles = get_all_articles_from_index()
    
    if not articles:
        logger.warning("âŒ æš‚æ— æ–‡ç« å¯åˆ†æ")
        return
    
    # åˆ—å‡ºæ–‡ç« 
    logger.info("\nğŸ“° å¯åˆ†æçš„æ–‡ç« åˆ—è¡¨:")
    for i, article in enumerate(articles, 1):
        logger.info(f"{i}. {article.get('title', 'N/A')[:50]}")
    
    try:
        choice = int(input("\nè¯·é€‰æ‹©æ–‡ç« ç¼–å·: ").strip())
        if 1 <= choice <= len(articles):
            article = articles[choice - 1]
        else:
            logger.warning("âŒ æ— æ•ˆçš„ç¼–å·")
            return
    except ValueError:
        logger.warning("âŒ è¯·è¾“å…¥æ­£ç¡®çš„ç¼–å·")
        return
    
    # è¯»å–æ–‡ç« æ•°æ®
    articles_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'articles')
    filename = article.get('filename', '')
    
    if not filename:
        logger.warning("âŒ æ–‡ç« æ–‡ä»¶åä¸¢å¤±")
        return
    
    filepath = os.path.join(articles_dir, filename)
    
    if not os.path.exists(filepath):
        logger.warning(f"âŒ æ–‡ç« æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            news_data = json.load(f)
    except Exception as e:
        logger.error(f"âŒ è¯»å–æ–‡ç« å¤±è´¥: {str(e)}")
        return
    
    # è·å–ç”¨æˆ·èµ„æ–™
    user_profile = config.get('user_profile', {})
    user_profile_str = json.dumps(user_profile, ensure_ascii=False)
    
    logger.info(f"\nğŸ”„ æ­£åœ¨åˆ†æ: {article.get('title', 'N/A')[:50]}...")
    
    # æ£€æµ‹æ–‡ä»¶æ˜¯å¦å·²åˆ†æè¿‡ï¼ˆé¿å…é‡å¤åˆ†æï¼‰
    source_filename = os.path.basename(filepath)
    if recorder.has_analysis(source_filename):
        logger.info(f"âœ… æ–‡ä»¶å·²å®Œæ•´åˆ†æè¿‡: {source_filename}")
        analysis_record = recorder.get_analysis_record(source_filename)
        if analysis_record:
            logger.info(f"   ğŸ“‹ åˆ†ææ—¶é—´: {analysis_record.get('timestamp')}")
            logger.info(f"   ğŸ“Š ç›¸å…³æ€§è¯„åˆ†: {analysis_record.get('relevance_score')}")
            logger.info(f"   ğŸ“ æ–°é—»æ ‡é¢˜: {analysis_record.get('news_title')}")
        logger.info("â­ï¸  è·³è¿‡åˆ†æï¼ˆä½¿ç”¨å·²æœ‰ç»“æœï¼‰")
        return
    
    logger.info(f"ğŸ†• é¦–æ¬¡åˆ†ææ­¤æ–‡ä»¶: {source_filename}")
    
    # å¤„ç†å·¥ä½œæµ
    result_json = handler.process_workflow(user_profile_str, filepath)
    result = json.loads(result_json)
    
    if result.get('status') == 'success':
        logger.info("âœ… åˆ†ææˆåŠŸ")
        
        analysis_data = result.get('data', {})
        logger.info(f"\nğŸ“Š åˆ†æç»“æœ:")
        logger.info(f"   æ ‡é¢˜: {analysis_data.get('title', 'N/A')}")
        logger.info(f"   æ‘˜è¦: {analysis_data.get('summary', 'N/A')[:100]}...")
        logger.info(f"   ç›¸å…³æ€§è¯„åˆ†: {analysis_data.get('relevance_score', 'N/A')}/10")
        logger.info(f"   è¯„åˆ†åŸå› : {analysis_data.get('relevance_reason', 'N/A')}")
        
        if result.get('dify_response_id'):
            logger.info(f"   Dify Response ID: {result.get('dify_response_id')}")
        
        # è®°å½•åˆ†æç»“æœ
        try:
            record_path = recorder.record_analysis(
                user_profile=user_profile,
                news_data=news_data,
                analysis_result=analysis_data,
                news_file_path=filepath
            )
            logger.info(f"\nâœ… åˆ†æç»“æœå·²è®°å½•: {record_path}")
        except Exception as e:
            logger.error(f"âŒ è®°å½•åˆ†æç»“æœå¤±è´¥: {str(e)}")
    
    elif result.get('status') == 'pending_analysis':
        logger.info("â³ Dify æœªå¯ç”¨ï¼Œå·¥ä½œæµè¾“å…¥å·²éªŒè¯...")
        logger.info("ğŸ“ è¾“å…¥å·²å‡†å¤‡å¥½ï¼Œè¯·åœ¨ Dify ä¸­é…ç½® API Key å¹¶è¿è¡Œå·¥ä½œæµå¤„ç†")
        
        # è¯¢é—®æ˜¯å¦ä¿å­˜è¾“å…¥æ•°æ®
        save = input("\næ˜¯å¦ä¿å­˜æ­¤åˆ†æçš„è¾“å…¥æ•°æ®ç”¨äº Dify? (y/n): ").strip().lower()
        if save == 'y':
            # ä¿å­˜ä¸´æ—¶è¾“å…¥æ•°æ®
            temp_input = {
                'user_profile': user_profile,
                'news': {
                    'title': news_data.get('title'),
                    'content': news_data.get('content'),
                    'source': news_data.get('source'),
                    'publish_date': news_data.get('publish_date')
                }
            }
            
            temp_path = os.path.join(
                os.path.dirname(__file__), 'ai', 'test_data',
                f"input_{article.get('title', 'untitled')[:30]}.json"
            )
            os.makedirs(os.path.dirname(temp_path), exist_ok=True)
            
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(temp_input, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… è¾“å…¥æ•°æ®å·²ä¿å­˜: {temp_path}")
    
    elif result.get('status') == 'error':
        logger.error(f"âŒ åˆ†æå¤±è´¥: {result.get('message')}")
        if result.get('errors'):
            for error in result.get('errors', []):
                logger.error(f"   - {error}")
    
    else:
        # å¦‚æœæœ‰åˆ†æç»“æœï¼Œè®°å½•å®ƒ
        if 'data' in result:
            try:
                record_path = recorder.record_analysis(
                    user_profile=user_profile,
                    news_data=news_data,
                    analysis_result=result.get('data', {}),
                    news_file_path=filepath
                )
                logger.info(f"âœ… åˆ†æç»“æœå·²è®°å½•: {record_path}")
            except Exception as e:
                logger.error(f"âŒ è®°å½•åˆ†æç»“æœå¤±è´¥: {str(e)}")


def _analyze_all_articles(handler, recorder, config):
    """æ‰¹é‡åˆ†ææ‰€æœ‰æ–‡ç« """
    articles = get_all_articles_from_index()
    
    if not articles:
        logger.warning("âŒ æš‚æ— æ–‡ç« å¯åˆ†æ")
        return
    
    logger.info(f"\nğŸš€ å‡†å¤‡æ‰¹é‡åˆ†æ {len(articles)} ç¯‡æ–‡ç« ...")
    
    user_profile = config.get('user_profile', {})
    user_profile_str = json.dumps(user_profile, ensure_ascii=False)
    
    articles_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'articles')
    
    processed = 0
    skipped = 0
    failed = 0
    
    for idx, article in enumerate(articles, 1):
        filename = article.get('filename', '')
        title = article.get('title', '')
        
        if not filename:
            logger.warning(f"[{idx}/{len(articles)}] â­ï¸  è·³è¿‡ï¼ˆæ— æ–‡ä»¶åï¼‰: {title[:30]}...")
            skipped += 1
            continue
        
        filepath = os.path.join(articles_dir, filename)
        
        if not os.path.exists(filepath):
            logger.warning(f"[{idx}/{len(articles)}] â­ï¸  è·³è¿‡ï¼ˆæ–‡ä»¶ä¸å­˜åœ¨ï¼‰: {title[:30]}...")
            skipped += 1
            continue
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                news_data = json.load(f)
        except Exception as e:
            logger.error(f"[{idx}/{len(articles)}] âŒ è¯»å–å¤±è´¥: {title[:30]}... ({str(e)})")
            failed += 1
            continue
        
        logger.info(f"[{idx}/{len(articles)}] ğŸ”„ åˆ†æä¸­: {title[:30]}...")
        
        # æ£€æµ‹æ–‡ä»¶æ˜¯å¦å·²åˆ†æè¿‡
        source_filename = os.path.basename(filepath)
        if recorder.has_analysis(source_filename):
            logger.info(f"[{idx}/{len(articles)}] âœ… å·²åˆ†æï¼ˆè·³è¿‡ï¼‰: {title[:30]}...")
            skipped += 1
            continue
        
        try:
            result_json = handler.process_workflow(user_profile_str, filepath)
            result = json.loads(result_json)
            
            if result.get('status') == 'success':
                recorder.record_analysis(
                    user_profile=user_profile,
                    news_data=news_data,
                    analysis_result=result.get('data', {}),
                    news_file_path=filepath
                )
                logger.info(f"[{idx}/{len(articles)}] âœ… æˆåŠŸ")
                processed += 1
            elif result.get('status') == 'pending_analysis':
                logger.info(f"[{idx}/{len(articles)}] â³ ç­‰å¾… Dify å¤„ç†")
                failed += 1
            else:
                logger.error(f"[{idx}/{len(articles)}] âŒ {result.get('message', 'åˆ†æå¤±è´¥')}")
                failed += 1
        
        except Exception as e:
            logger.error(f"[{idx}/{len(articles)}] âŒ åˆ†æå¼‚å¸¸: {str(e)}")
            failed += 1
    
    logger.info(f"\nğŸ“Š æ‰¹é‡åˆ†æå®Œæˆ:")
    logger.info(f"   âœ… æˆåŠŸ: {processed}")
    logger.info(f"   â­ï¸  è·³è¿‡: {skipped}")
    logger.info(f"   âŒ å¤±è´¥: {failed}")


def _view_analysis_history(recorder):
    """æŸ¥çœ‹åˆ†æå†å²"""
    logger.info("\nğŸ“‹ åˆ†æå†å²è®°å½•")
    
    history = recorder.get_analysis_history(limit=20)
    
    if not history:
        logger.info("ğŸ“­ æš‚æ— åˆ†æè®°å½•")
        return
    
    logger.info(f"æœ€è¿‘ {len(history)} æ¡è®°å½•:")
    for i, record in enumerate(history, 1):
        logger.info(f"{i}. {record.get('timestamp')} - {record.get('news_title', 'N/A')[:40]}")
        logger.info(f"   ç›¸å…³æ€§è¯„åˆ†: {record.get('relevance_score', 'N/A')}/10")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = recorder.get_statistics()
    logger.info(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    logger.info(f"   æ€»åˆ†ææ•°: {stats.get('total_analyses', 0)}")
    logger.info(f"   å¹³å‡ç›¸å…³æ€§è¯„åˆ†: {stats.get('average_relevance_score', 0):.1f}/10")
    
    distribution = stats.get('score_distribution', {})
    logger.info(f"   é«˜ç›¸å…³æ€§ (8-10): {distribution.get('high', 0)}")
    logger.info(f"   ä¸­ç›¸å…³æ€§ (5-7): {distribution.get('medium', 0)}")
    logger.info(f"   ä½ç›¸å…³æ€§ (0-4): {distribution.get('low', 0)}")


def _check_analysis_validity(recorder):
    """æ£€æŸ¥åˆ†æç»“æœçš„æœ‰æ•ˆæ€§"""
    logger.info("\nğŸ” æ£€æŸ¥åˆ†ææœ‰æ•ˆæ€§")
    
    history = recorder.get_analysis_history(limit=50)
    
    if not history:
        logger.info("ğŸ“­ æš‚æ— åˆ†æè®°å½•")
        return
    
    outdated = recorder.find_outdated_analyses()
    
    if outdated:
        logger.warning(f"âš ï¸ å‘ç° {len(outdated)} ä¸ªå¯èƒ½è¿‡æœŸçš„åˆ†æç»“æœ:")
        for record in outdated:
            logger.warning(f"   - {record.get('filename')}")
            logger.warning(f"     åŸå› : {record.get('details')}")
    else:
        logger.info("âœ… æ‰€æœ‰åˆ†æç»“æœéƒ½æ˜¯æœ€æ–°çš„")



def main():
    logger.info("=" * 40)
    logger.info("ğŸ“° SZTU æ–°é—»çˆ¬è™«")
    logger.info("=" * 40)
    
    while True:
        logger.info("\nè¯·é€‰æ‹©æ“ä½œ:")
        logger.info("1. çˆ¬å–æ–°é—»æ ‡é¢˜å’Œé“¾æ¥ï¼ˆä¿å­˜ä¸º JSONï¼‰")
        logger.info("2. çˆ¬å–å®Œæ•´æ–‡ç« ï¼ˆæ ‡é¢˜ã€å†…å®¹ã€æ—¶é—´ç­‰ï¼‰")
        logger.info("3. æŸ¥çœ‹å·²çˆ¬å–çš„æ–°é—»")
        logger.info("4. æ ¹æ® URL æŸ¥è¯¢æ–‡ç« ")
        logger.info("5. æ ¹æ®æ ‡é¢˜æœç´¢æ–‡ç« ")
        logger.info("6. ğŸ¤– å¯åŠ¨ AI åˆ†æ")
        logger.info("7. å¯åŠ¨ Web æµè§ˆç•Œé¢")
        logger.info("8. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-8): ").strip()
        
        if choice == "1":
            fetch_news_json()
        elif choice == "2":
            fetch_news()
        elif choice == "3":
            list_articles()
        elif choice == "4":
            search_by_url()
        elif choice == "5":
            search_by_title()
        elif choice == "6":
            analyze_news_with_ai()
        elif choice == "7":
            import subprocess
            import sys
            logger.info("ğŸš€ å¯åŠ¨ Streamlit Web åº”ç”¨...")
            logger.info("ğŸ“± è®¿é—®åœ°å€: http://localhost:8501")
            subprocess.run([
                sys.executable, "-m", "streamlit", "run",
                os.path.join(os.path.dirname(__file__), "streamlit_app.py")
            ])
        elif choice == "8":
            logger.info("ğŸ‘‹ å†è§ï¼")
            break
        else:
            logger.warning("âŒ æ— æ•ˆçš„é€‰é¡¹")


if __name__ == "__main__":
    main()
